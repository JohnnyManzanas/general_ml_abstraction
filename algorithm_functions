#Abstraction of one hot encoding
def one_hot(train,y):
  from sklearn.preprocessing import OneHotEncoder
  from sklearn.metrics import mean_absolute_error
  from sklearn.model_selection import train_test_split
  from sklearn.ensemble import RandomForestRegressor
  # Get list of categorical variables
  categorical_columns = (train.dtypes == 'object')
  object_cols = list(categorical_columns[categorical_columns].index)
  X = train.drop(y,axis=1)
  # Apply one-hot encoder to each column with categorical data
  OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
  OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train[object_cols].astype('str')))
  # One-hot encoding removed index; put it back
  OH_cols_train.index = X.index
  # Remove categorical columns (will replace with one-hot encoding)
  num_X_train = X.drop(object_cols, axis=1)
  # Add one-hot encoded columns to numerical features
  OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
  # Ensure all columns have string type
  OH_X_train.columns = OH_X_train.columns.astype(str)
  return OH_X_train

#Abstraction of an XGB regressor
def xgb(train,y):
  from sklearn.model_selection import train_test_split
  from xgboost import XGBRegressor
  X_train, X_valid,y_train, y_valid = train_test_split(train,y,random_state=104,test_size=0.2,shuffle=True)
  xgb_model = XGBRegressor(n_estimators=500,learning_rate=0.05)
  xgb_model.fit(X_train, y_train, early_stopping_rounds=5,eval_set=[(X_valid, y_valid)],verbose=False)
  return xgb_model

#Abstraction of a random forest regressor
def random_forest(train, y):
  from sklearn.ensemble import RandomForestRegressor
  forest_model = RandomForestRegressor(n_estimators=100, random_state=0)
  forest_model.fit(train, y)
  return forest_model

#Abstraction of a neural network
def neural_network(X_train,y):
  import tensorflow as tf
  from tensorflow import keras
  from sklearn.preprocessing import MinMaxScaler
  # Normalize the data
  scaler = MinMaxScaler()
  X_train_nn = scaler.fit_transform(X_train)
  # Create the model
  neural_network = tf.keras.Sequential([
      tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
      tf.keras.layers.Dense(32, activation='relu'),
      tf.keras.layers.Dense(1)
  ])
  # Compile the model, adjust hyperparameters to taste
  neural_network.compile(optimizer='AdaDelta',loss='mse',metrics=['mae'])
  # Early stopping callback to stop training when loss stops improving
  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)
  # Train the model
  neural_network.fit(X_train_nn, y, epochs=500, callbacks=[early_stopping])
  return neural_network
