#Abstraction of an XGB regressor
def xgb(train,y):
  from sklearn.model_selection import train_test_split
  from xgboost import XGBRegressor
  X_train, X_valid,y_train, y_valid = train_test_split(train,y,random_state=104,test_size=0.2,shuffle=True)
  xgb_model = XGBRegressor(n_estimators=500,learning_rate=0.05)
  xgb_model.fit(X_train, y_train, early_stopping_rounds=5,eval_set=[(X_valid, y_valid)],verbose=False)
  return xgb_model

#Abstraction of a random forest regressor
def random_forest(train, y):
  from sklearn.ensemble import RandomForestRegressor
  forest_model = RandomForestRegressor(n_estimators=100, random_state=0)
  forest_model.fit(train, y)
  return forest_model

#Abstraction of a neural network
def neural_network(X_train,y):
  import tensorflow as tf
  from tensorflow import keras
  from sklearn.preprocessing import MinMaxScaler
  # Normalize the data
  scaler = MinMaxScaler()
  X_train_nn = scaler.fit_transform(X_train)
  # Create the model
  neural_network = tf.keras.Sequential([
      tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
      tf.keras.layers.Dense(32, activation='relu'),
      tf.keras.layers.Dense(1)
  ])
  # Compile the model, adjust hyperparameters to taste
  neural_network.compile(optimizer='AdaDelta',loss='mse',metrics=['mae'])
  # Early stopping callback to stop training when loss stops improving
  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)
  # Train the model
  neural_network.fit(X_train_nn, y, epochs=500, callbacks=[early_stopping])
  return neural_network
